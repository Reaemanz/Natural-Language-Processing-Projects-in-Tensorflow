{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QnXzcZvdc-r6"
   },
   "source": [
    "In this notebook we will demostrate how to perform tokenization,stemming,lemmatization and pos_tagging using libraries like [spacy](https://spacy.io/) and [nltk](https://www.nltk.org/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.19.5\n",
      "  Using cached numpy-1.19.5-cp36-cp36m-win_amd64.whl (13.2 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.19.5\n",
      "Collecting nltk==3.2.5\n",
      "  Using cached nltk-3.2.5-py3-none-any.whl\n",
      "Requirement already satisfied: six in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from nltk==3.2.5) (1.16.0)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.2.5\n",
      "Collecting spacy==2.2.4\n",
      "  Using cached spacy-2.2.4-cp36-cp36m-win_amd64.whl (9.9 MB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.5-cp36-cp36m-win_amd64.whl (109 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Using cached wasabi-0.8.2-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy==2.2.4) (1.19.5)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Using cached srsly-1.0.5-cp36-cp36m-win_amd64.whl (176 kB)\n",
      "Collecting tqdm<5.0.0,>=4.38.0\n",
      "  Using cached tqdm-4.61.2-py2.py3-none-any.whl (76 kB)\n",
      "Collecting thinc==7.4.0\n",
      "  Using cached thinc-7.4.0-cp36-cp36m-win_amd64.whl (2.1 MB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Using cached blis-0.4.1-cp36-cp36m-win_amd64.whl (5.0 MB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.5-cp36-cp36m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy==2.2.4) (52.0.0.post20210125)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting requests<3.0.0,>=2.13.0\n",
      "  Using cached requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.5-cp36-cp36m-win_amd64.whl (20 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.10.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.5.0)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Using cached urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n",
      "Collecting charset-normalizer~=2.0.0\n",
      "  Using cached charset_normalizer-2.0.3-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2021.5.30)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.2-py3-none-any.whl (59 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy==2.2.4) (0.4.4)\n",
      "Installing collected packages: murmurhash, cymem, wasabi, urllib3, tqdm, srsly, preshed, plac, idna, charset-normalizer, catalogue, blis, thinc, requests, spacy\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 charset-normalizer-2.0.3 cymem-2.0.5 idna-3.2 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 requests-2.26.0 spacy-2.2.4 srsly-1.0.5 thinc-7.4.0 tqdm-4.61.2 urllib3-1.26.6 wasabi-0.8.2\n"
     ]
    }
   ],
   "source": [
    "# To install only the requirements of this notebook, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "!pip install numpy==1.19.5\n",
    "!pip install nltk==3.2.5\n",
    "!pip install spacy==2.2.4\n",
    "\n",
    "# ===========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the requirements for the entire chapter, uncomment the lines below and run this cell\n",
    "\n",
    "# ===========================\n",
    "\n",
    "# try :\n",
    "#     import google.colab\n",
    "#     !curl https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch2/ch2-requirements.txt | xargs -n 1 -L 1 pip install\n",
    "# except ModuleNotFoundError :\n",
    "#     !pip install -r \"ch2-requirements.txt\"\n",
    "\n",
    "# ==========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R3xEmJpRc5r8"
   },
   "outputs": [],
   "source": [
    "#This will be our corpus which we will work on\n",
    "corpus_original = \"Need to finalize the demo corpus which will be used for this notebook and it should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\"\n",
    "corpus = \"Need to finalize the demo corpus which will be used for this notebook & should be done soon !!. It should be done by the ending of this month. But will it? This notebook has been run 4 times !!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "KHh_33IopPTf",
    "outputId": "fa12e7e4-aeb3-4053-be10-3cadad90d094"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run 4 times !!\n"
     ]
    }
   ],
   "source": [
    "#lower case the corpus\n",
    "corpus = corpus.lower()\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "3yaGf8RiqgBM",
    "outputId": "859abb8b-3a34-4e23-bd8e-963520fb6ed3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook & should be done soon !!. it should be done by the ending of this month. but will it? this notebook has been run  times !!\n"
     ]
    }
   ],
   "source": [
    "#removing digits in the corpus\n",
    "import re\n",
    "corpus = re.sub(r'\\d+','', corpus)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "v5Q--GItqzfu",
    "outputId": "82fec440-1251-4ba1-cdf4-f1cab3c2a607"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need to finalize the demo corpus which will be used for this notebook  should be done soon  it should be done by the ending of this month but will it this notebook has been run  times \n"
     ]
    }
   ],
   "source": [
    "#removing punctuations\n",
    "import string\n",
    "corpus = corpus.translate(str.maketrans('', '', string.punctuation))\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "zmANqee9rK4N",
    "outputId": "6105b616-e770-409a-88b0-3f23dd3ffd72"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing trailing whitespaces\n",
    "corpus = ' '.join([token for token in corpus.split()])\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.2.5\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
      "Requirement already satisfied: spacy>=2.2.2 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: thinc==7.4.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.61.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (52.0.0.post20210125)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.6.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\kumar apurv\\anaconda3\\envs\\ch2\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.4)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py): started\n",
      "  Building wheel for en-core-web-sm (setup.py): finished with status 'done'\n",
      "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.2.5-py3-none-any.whl size=12011738 sha256=e6016cd0d30eb355016074132611edd6fcb4f5684b64c69be8431821619dfbc3\n",
      "  Stored in directory: C:\\Users\\KUMARA~1\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-ezw119fz\\wheels\\b5\\94\\56\\596daa677d7e91038cbddfcf32b591d0c915a1b3a3e3d3c79d\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.2.5\n",
      "[+] Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfJx3MnVj_ph"
   },
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "OUz580k2sMqf",
    "outputId": "da21bf1e-444b-4077-c823-e58b4986a35f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\KUMAR\n",
      "[nltk_data]     APURV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\KUMAR\n",
      "[nltk_data]     APURV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "\n",
      "NLTK\n",
      "Tokenized corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords: ['need', 'finalize', 'demo', 'corpus', 'used', 'notebook', 'done', 'soon', 'done', 'ending', 'month', 'notebook', 'run', 'times']\n",
      "\n",
      "Spacy:\n",
      "Tokenized Corpus: ['need', 'to', 'finalize', 'the', 'demo', 'corpus', 'which', 'will', 'be', 'used', 'for', 'this', 'notebook', 'should', 'be', 'done', 'soon', 'it', 'should', 'be', 'done', 'by', 'the', 'ending', 'of', 'this', 'month', 'but', 'will', 'it', 'this', 'notebook', 'has', 'been', 'run', 'times']\n",
      "Tokenized corpus without stopwords ['need', 'finalize', 'demo', 'corpus', 'notebook', 'soon', 'ending', 'month', 'notebook', 'run', 'times']\n",
      "Difference between NLTK and spaCy output:\n",
      " {'done', 'used'}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "##NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words_nltk = set(stopwords.words('english'))\n",
    "\n",
    "tokenized_corpus_nltk = word_tokenize(corpus)\n",
    "print(\"\\nNLTK\\nTokenized corpus:\",tokenized_corpus_nltk)\n",
    "tokenized_corpus_without_stopwords = [i for i in tokenized_corpus_nltk if not i in stop_words_nltk]\n",
    "print(\"Tokenized corpus without stopwords:\",tokenized_corpus_without_stopwords)\n",
    "\n",
    "\n",
    "##SPACY \n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import spacy\n",
    "spacy_model = spacy.load('en_core_web_sm')\n",
    "\n",
    "stopwords_spacy = spacy_model.Defaults.stop_words\n",
    "print(\"\\nSpacy:\")\n",
    "tokenized_corpus_spacy = word_tokenize(corpus)\n",
    "print(\"Tokenized Corpus:\",tokenized_corpus_spacy)\n",
    "tokens_without_sw= [word for word in tokenized_corpus_spacy if not word in stopwords_spacy]\n",
    "\n",
    "print(\"Tokenized corpus without stopwords\",tokens_without_sw)\n",
    "\n",
    "\n",
    "print(\"Difference between NLTK and spaCy output:\\n\",\n",
    "      set(tokenized_corpus_without_stopwords)-set(tokens_without_sw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRH_ltkD-HpA"
   },
   "source": [
    "Notice the difference output after stopword removal using nltk and spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tGcwD1JlkEao"
   },
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "ibEpzcv0sdW8",
    "outputId": "18f77b85-3a8e-4e89-df28-3bd6342ac594"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Stemming:\n",
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook has been run times\n",
      "After Stemming:\n",
      "need to final the demo corpu which will be use for thi notebook should be done soon it should be done by the end of thi month but will it thi notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer= PorterStemmer()\n",
    "\n",
    "print(\"Before Stemming:\")\n",
    "print(corpus)\n",
    "\n",
    "print(\"After Stemming:\")\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(stemmer.stem(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Wy6cwvYkJeR"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "27KvL4ZE-fqJ",
    "outputId": "d8b6778f-79b7-4dd4-8832-da29d75dc3a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\KUMAR\n",
      "[nltk_data]     APURV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "need to finalize the demo corpus which will be used for this notebook should be done soon it should be done by the ending of this month but will it this notebook ha been run time "
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "\n",
    "for word in tokenized_corpus_nltk:\n",
    "    print(lemmatizer.lemmatize(word),end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h8uCGA8ukMfQ"
   },
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "kZqBxLDz-6cu",
    "outputId": "a8503608-0352-4c00-82fe-789d874b5655"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Tagging using spacy:\n",
      "Need : VERB\n",
      "to : PART\n",
      "finalize : VERB\n",
      "the : DET\n",
      "demo : NOUN\n",
      "corpus : NOUN\n",
      "which : DET\n",
      "will : VERB\n",
      "be : AUX\n",
      "used : VERB\n",
      "for : ADP\n",
      "this : DET\n",
      "notebook : NOUN\n",
      "and : CCONJ\n",
      "it : PRON\n",
      "should : VERB\n",
      "be : AUX\n",
      "done : VERB\n",
      "soon : ADV\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      ". : PUNCT\n",
      "It : PRON\n",
      "should : VERB\n",
      "be : AUX\n",
      "done : VERB\n",
      "by : ADP\n",
      "the : DET\n",
      "ending : NOUN\n",
      "of : ADP\n",
      "this : DET\n",
      "month : NOUN\n",
      ". : PUNCT\n",
      "But : CCONJ\n",
      "will : VERB\n",
      "it : PRON\n",
      "? : PUNCT\n",
      "This : DET\n",
      "notebook : NOUN\n",
      "has : AUX\n",
      "been : AUX\n",
      "run : VERB\n",
      "4 : NUM\n",
      "times : NOUN\n",
      "! : PUNCT\n",
      "! : PUNCT\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\KUMAR APURV\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "POS Tagging using NLTK:\n",
      "[('Need', 'NN'),\n",
      " ('to', 'TO'),\n",
      " ('finalize', 'VB'),\n",
      " ('the', 'DT'),\n",
      " ('demo', 'NN'),\n",
      " ('corpus', 'NN'),\n",
      " ('which', 'WDT'),\n",
      " ('will', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('used', 'VBN'),\n",
      " ('for', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('and', 'CC'),\n",
      " ('it', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('soon', 'RB'),\n",
      " ('!', '.'),\n",
      " ('!', '.'),\n",
      " ('.', '.'),\n",
      " ('It', 'PRP'),\n",
      " ('should', 'MD'),\n",
      " ('be', 'VB'),\n",
      " ('done', 'VBN'),\n",
      " ('by', 'IN'),\n",
      " ('the', 'DT'),\n",
      " ('ending', 'VBG'),\n",
      " ('of', 'IN'),\n",
      " ('this', 'DT'),\n",
      " ('month', 'NN'),\n",
      " ('.', '.'),\n",
      " ('But', 'CC'),\n",
      " ('will', 'MD'),\n",
      " ('it', 'PRP'),\n",
      " ('?', '.'),\n",
      " ('This', 'DT'),\n",
      " ('notebook', 'NN'),\n",
      " ('has', 'VBZ'),\n",
      " ('been', 'VBN'),\n",
      " ('run', 'VBN'),\n",
      " ('4', 'CD'),\n",
      " ('times', 'NNS'),\n",
      " ('!', '.'),\n",
      " ('!', '.')]\n"
     ]
    }
   ],
   "source": [
    "#POS tagging using spacy\n",
    "print(\"POS Tagging using spacy:\")\n",
    "doc = spacy_model(corpus_original)\n",
    "# Token and Tag\n",
    "for token in doc:\n",
    "    print(token,\":\", token.pos_)\n",
    "\n",
    "#pos tagging using nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "print(\"POS Tagging using NLTK:\")\n",
    "pprint(nltk.pos_tag(word_tokenize(corpus_original)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zWdmz6lFkpEI"
   },
   "source": [
    "There are various other libraries you can use to perform these common pre-processing steps"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tokenization_Stemming_lemmatization_stopword_postagging.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
